# -*- coding: utf-8 -*-
"""LVADSUSR158_Jeno_AnomalyFA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FoJ6Y784EyiwSpTzWrJGETdx0W5KyIOG
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler,OneHotEncoder, LabelEncoder
from sklearn.ensemble import IsolationForest
warnings.filterwarnings('ignore')

df=pd.read_csv("/content/anomaly_train.csv")
df

df.info()

df.shape

df.describe()

df.dtypes

df.isna().sum()

df.duplicated().sum()

X = df[['Amount','Time','User']]

model = IsolationForest(n_estimators=100,contamination=0.1)
model.fit(X)
y_pred = model.predict(X)
df['anomaly_score'] = model.decision_function(X)
df['anomaly'] = y_pred
anomalies = df.loc[df['anomaly'] == -1]

anomalies

anomalies.shape

plt.scatter(df['Amount'],df['anomaly_score'],label='normal')
plt.scatter(anomalies['Amount'],anomalies['anomaly_score'],label='anomaly')
plt.xlabel('Amount')
plt.ylabel('anomaly_score')
plt.legend()
plt.show()

df['is_anomaly'] = df['anomaly'].map({-1:"yes",1:"no"})
df

df['is_anomaly'].value_counts()

"""Analysis:
Since the data contained no null and duplicate values no data cleaning was required. Apllying the Isolation Forest algorithm
10% of the dataset was determined to be anomalies for the financial institution.
"""